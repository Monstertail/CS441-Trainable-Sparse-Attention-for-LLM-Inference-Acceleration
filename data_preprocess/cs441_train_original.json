{"question": "True or false: With different sets of M samples (test set), we would probably get the same error measurement when estimating a model's error?", "answer": "We rely on the test set to estimate the true error. ** False. There would be some variance in the error measurement, since we'd be measuring based on a different set of samples. As an extreme example, if M=1 then for some test sets, the average error would be 0 and for others it would be 1.\n#### False"}
{"question": "True or false: If we increase the test set size M, we should get a more accurate estimate of the model's error?", "answer": "Consider the relationship between sample size and variance. ** True. Increasing the sample size decreases the error of the estimate of the mean. Specifically, the var(mean(X)) = 1/M var(X), where M is the sample size used to estimate the mean.\n#### True"}
{"question": "True or false: If we increase the training set size N but do not change the test set, we'd expect the test error to be unchanged.", "answer": "Consider how training size affects model parameters. ** False. Test error should go down because we can fit parameters better with more training samples (or have more samples to represent the full distribution in the case of KNN).\n#### False"}
{"question": "True or false: The expected error does not depend on the test set size M, but it does depend on the training set size N.", "answer": "The expected error is an intrinsic property of the model trained on N samples. ** True.\n#### True"}
{"question": "Which of these assumptions are implied by the use of a Euclidean or L2 distance measure for a KNN classifier? (Select all that apply)\na. Each feature dimension is equally important\nb. The feature dimensions have comparable scales\nc. Each feature dimension has roughly the same mean", "answer": "Euclidean distance sums the squared differences across all dimensions equally. ** This implies that each feature dimension is equally important (a) and that feature dimensions have comparable scales (b). If scales differ significantly, larger scales will dominate the distance metric.\n#### a, b"}
{"question": "Which of the following are true of the nearest neighbor algorithm? (Select all that apply)\na. Fast inference\nb. Fast training\nc. Can be applied if only one sample per class is available\nd. Is not commonly used in practice\ne. Is most powerful when used in combination with feature learning", "answer": "Consider the computational requirements and data needs of KNN. ** Nearest neighbor has fast training (just storing data), can work with one sample per class (1-shot), and is powerful with learned features. Inference is slow because it requires comparing to all training data.\n#### b, c, e"}
{"question": "True or false: The basic idea of K-means is to assign each point to the nearest of the established K centers.", "answer": "This describes the assignment step of K-means. ** True.\n#### True"}
{"question": "True or false: The problem with a very structured distribution of points is that it can make the K-means algorithm not converge to a final clustering of all the data points.", "answer": "Consider the convergence properties of K-means. ** False. K-means is guaranteed to always converge (though potentially to a local minimum).\n#### False"}
{"question": "True or false: High-dimensional data points make K-means iterate more times before achieving a sufficiently good clustering compared to low-dimensional data points.", "answer": "Does dimension directly dictate iteration count? ** False. This is not true because, in general, the number of iterations will depend on the number of clusters and on the number of data points, and not on the dimension of the data points itself.\n#### False"}
{"question": "True or false: High-dimensional data points increase the computational cost spent when running K-means.", "answer": "Consider the cost of distance calculations. ** True. K-means can have different computational costs per iteration since computing Euclidean distances between points needs more operations as the dimensions grow.\n#### True"}
{"question": "True or false: K-means is a deterministic algorithm, but it is sensitive to the initialization of the centers for each cluster.", "answer": "Consider the initialization step. ** True. The final clusters depend heavily on where the initial centers are placed.\n#### True"}
{"question": "True or false: The problem with clustering methods such as K-means and hierarchical K-means is that they can be sensitive to the local connectivity of the data.", "answer": "Think about the 'smiley face' dataset example. ** False. In fact, it is quite the opposite. They are often unable to capture local connectivity (like the smiley face example) because they rely on global distance metrics rather than manifold structure.\n#### False"}
{"question": "True or false: If we know that some attributes are more important than others, we can still use K-means and expect a good clustering without modification.", "answer": "How does K-means weight features? ** False. The problem is that since we calculate the Euclidean distance between two points, K-means weights every feature or attribute equally. We would need to use weighting to avoid this problem.\n#### False"}
{"question": "If you have vectors with continuous values that you want to group into different categories, how do clustering methods help?", "answer": "Vectors with continuous values are inherently difficult to count/group. ** Clustering allows one to capture similarities among them by assigning a cluster-ID, claiming that all vectors with the same cluster-ID share similar attribute values. The similarity measure (e.g., Euclidean distance) handles the continuous nature.\n#### Capture similarities"}
{"question": "Imagine you have a group of unlabelled data and used two clustering algorithms. How would you compare the results of both clustering methods?", "answer": "We need a metric to compare the quality or alignment of clusters. ** In general, you could use RMSE to compare clusterings. If you want clusters to correspond to a certain labeling, you can compute the purity measure for each clustering result on a labeled subset and choose the clustering that scores the largest value.\n#### RMSE or Purity measure"}
{"question": "For the following description, select the distance measure that best corresponds (L2, L1, Mahalanobis): Each dimension has the same scale, distance can be dominated by large differences in one dimension.", "answer": "L2 (Euclidean) distance involves squaring differences, making it sensitive to outliers/large differences. ** L2.\n#### L2"}
{"question": "For the following description, select the distance measure that best corresponds (L2, L1, Mahalanobis): Each dimension has the same scale, sensitive to the sum of absolute differences.", "answer": "L1 (Manhattan) distance sums absolute differences. ** L1.\n#### L1"}
{"question": "True or false: PCA is based on computing eigenvectors of the empirical covariance matrix, but a problem is that sometimes such eigenvectors can have imaginary entries.", "answer": "Consider the properties of the covariance matrix. ** False. Eigenvectors and eigenvalues are always real because the empirical covariance matrix is symmetric.\n#### False"}
{"question": "True or false: When using PCA, the eigenvectors of the empirical covariance matrix are able to capture discriminative features from the data.", "answer": "Does PCA care about class labels or discrimination? ** False. PCA does not capture discriminative features since it does not care about the nature of the data or saliency; it simply captures directions with the most variance.\n#### False"}
{"question": "True or false: In every application of PCA, the largest components (associated with largest eigenvalues) are always the most important to preserve.", "answer": "Is variance always equal to information/importance? ** False. For example, in eigenfaces, the largest components might just encode lighting changes rather than facial features. Users might disregard large components if they contain irrelevant variance.\n#### False"}
{"question": "True or false: Multidimensional scaling (MDS) aims to preserve pairwise distances between points in the lower dimension.", "answer": "What is the objective function of MDS? ** True. MDS is designed to place points in a lower dimension such that their pairwise distances match the original distances as closely as possible.\n#### True"}
{"question": "True or false: ISOMAP defines a unique graph over which we calculate the distance between two points based on the shortest path between them.", "answer": "Is the graph construction unique? ** False. While the shortest path defines the distance, ISOMAP does not define a 'unique' graph; it is up to the user to construct the graph based on how many nearest neighbors (k) are considered.\n#### False"}
{"question": "True or false: t-SNE computes pairwise probability distributions and minimizes the KL divergence between distributions in original and new coordinates.", "answer": "Does t-SNE use KL divergence? ** True.\n#### True"}
{"question": "How would you choose the number of components when using PCA?", "answer": "We need to balance dimensionality reduction with information loss. ** Compute the cumulative variance. Plot the cumulative explained variance vs. the number of components. Choose K where the cumulative variance is high and adding more components provides only a small marginal increase (the elbow method).\n#### Cumulative variance plot"}
{"question": "Why might a researcher use PCA as a preliminary step before using MDS on a high-dimensional dataset?", "answer": "Consider the limitations of distance metrics in high dimensions. ** 1) In high dimensions, distance concentration can occur (points equidistant), so PCA introduces heterogeneity. 2) PCA is computationally faster in higher dimensions, reducing the cost before applying the more expensive MDS.\n#### Computational efficiency and distance heterogeneity"}
{"question": "Why does MDS often preserve the global shape (e.g., S-shape) of data, while t-SNE might not?", "answer": "Compare global vs. local structure preservation. ** MDS is designed to preserve global structure (pairwise distances between all points). t-SNE is designed to preserve local structure (neighbors), so it does not guarantee the preservation of global shapes like an S-curve.\n#### MDS preserves global structure; t-SNE preserves local structure"}
{"question": "How do L1 and L2 regularization complement each other in linear models?", "answer": "Compare the effects of Lasso (L1) and Ridge (L2). ** L1 allows for feature selection (setting weights to zero). L2 maintains smaller weights to avoid unconstrained solutions and overfitting. They can be combined in Elastic Net.\n#### L1 for feature selection, L2 for weight constraints"}
{"question": "Should you use Logistic Regression (LG) or Linear Regression (LR) to predict the likelihood that a company's stock price is overvalued?", "answer": "The target is a probability/binary state (overvalued or not). ** LG, since we are predicting a probability of a binary variable.\n#### Logistic Regression"}
{"question": "Should you use Logistic Regression (LG) or Linear Regression (LR) to predict the future earnings of a company based on history?", "answer": "The target is a continuous real value. ** LR, since we are predicting a real (continuous) value.\n#### Linear Regression"}
{"question": "True or false: Hyperparameters can be optimized using the training data via a special transformation of variables.", "answer": "Do hyperparameters depend on training data optimization directly? ** False. Hyperparameters are design parameters specified outside of training; they do not depend on the training data in the gradient update sense.\n#### False"}
{"question": "True or false: Cross-validation splits the training data to measure hyperparameter performance over different validation sets.", "answer": "What is the definition of CV? ** True. This is the definition of cross-validation.\n#### True"}
{"question": "True or false: When using linear regression with sufficient data, there is no need to have regularization.", "answer": "Does data volume eliminate the need for feature selection or weight control? ** False. Regularization benefits large datasets too; L1 helps with feature selection and L2 avoids large weights/overfitting, which are properties orthogonal to data size.\n#### False"}
{"question": "Why is linear regression vulnerable to outliers?", "answer": "Linear regression minimizes squared differences. ** An outlier is a point drastically different from the trend. Since LR minimizes squared error, a single outlier with a large error pulls the fitted line significantly towards itself to reduce that specific large error, degrading the fit for the rest of the points.\n#### Minimization of squared errors pulls the line toward outliers"}
{"question": "True or false: SVMs are more explainable than artificial neural networks.", "answer": "Compare the complexity of decision boundaries. ** True.\n#### True"}
{"question": "True or false: When training an SVM, we minimize the margin in order to improve generalization.", "answer": "What is the relationship between margin size and overfitting? ** False. Allowing a *larger* margin avoids overfitting. We maximize the margin.\n#### False"}
{"question": "True or false: Hinge loss increases quadratically depending on how far the misclassified point is from the boundary.", "answer": "Check the definition of Hinge loss. ** False. It increases linearly for misclassified points.\n#### False"}
{"question": "True or false: An advantage of SVMs is that we can use kernels to implement feature mapping without explicit mapping.", "answer": "This refers to the Kernel Trick. ** True.\n#### True"}
{"question": "True or false: Removing a support vector from the training data will never affect the decision boundary.", "answer": "Do support vectors define the boundary? ** False. Support vectors define the margin and boundary. Removing one is very likely to move the boundary.\n#### False"}
{"question": "What assumption does the Naive Bayes model make if there are two features x1 and x2?", "answer": "Naive Bayes assumes feature independence given the class y. ** P(x1,x2|y) = P(x1|y)P(x2|y).\n#### P(x1,x2|y) = P(x1|y)P(x2|y)"}
{"question": "True or false: If x1 is independent of x2, then x1 and x2 are conditionally independent given y.", "answer": "Check the definition of conditional independence vs marginal independence. ** False. If x1 and x2 both cause y, knowing y couples them (Explaining Away effect).\n#### False"}
{"question": "According to Bayes rule, what is P(y|x)?", "answer": "Apply Bayes theorem. ** P(y|x) = P(x|y)P(y)/P(x).\n#### P(x|y)P(y)/P(x)"}
{"question": "True or false: The EM algorithm is guaranteed to always converge to the global maximum.", "answer": "Does EM avoid local maxima? ** False. Though EM is guaranteed to converge, it can converge to local maxima.\n#### False"}
{"question": "True or false: K-means is an example of a hard EM algorithm.", "answer": "Compare K-means steps to EM steps. ** True.\n#### True"}
{"question": "True or false: In the EM algorithm, the likelihood of the observed data increases after each iteration.", "answer": "Is EM monotonic? ** True.\n#### True"}
{"question": "Given binary data x ~ Bernoulli(p), what is the MLE estimate of p?", "answer": "Derive the derivative of the log likelihood. ** p = sum(xi) / N.\n#### sum(xi)/N"}
{"question": "True or false: Using histograms to describe a PDF works better the higher the dimension of the data.", "answer": "Consider the curse of dimensionality and binning. ** False. The more dimensions, the more bins are needed, resulting in low counts per bin (sparse data).\n#### False"}
{"question": "Which method is best to approximate a PDF with two clearly differentiated modes: Gaussian or Mixture of Gaussians?", "answer": "Gaussian is unimodal. ** Mixture of Gaussians, because the two modes imply a bimodal distribution which a single Gaussian cannot fit well.\n#### Mixture of Gaussians"}
{"question": "True or false: The median filter is a robust filter because it is not affected by extreme values (outliers).", "answer": "Compare mean vs median sensitivity. ** True.\n#### True"}
{"question": "For an image with 'salt and pepper' noise (pixels jumping to black/white extremes), which filter should be used?", "answer": "Salt and pepper noise represents outliers. ** The median filter, as it is robust to outliers.\n#### Median filter"}
{"question": "For an image with Gaussian noise (granular distortion), which filter should be used?", "answer": "Gaussian noise affects pixels uniformly without extremes. ** A moving average filter.\n#### Moving average filter"}
{"question": "Why are decision boundaries in simple decision trees parallel to the axes?", "answer": "How does a standard decision tree split data? ** Because any classification decision/split is made with respect to only one feature at a time (e.g., x1 > c).\n#### Splits are based on single features"}
{"question": "Why is it good to use early stopping during the training of decision trees?", "answer": "What happens if a tree grows indefinitely? ** If not stopped, the tree may isolate every single data point into its own leaf, leading to overfitting.\n#### To prevent overfitting"}
{"question": "True or false: The idea of boosting is to combine multiple weak classifiers to obtain a strong one.", "answer": "Definition of boosting. ** True.\n#### True"}
{"question": "True or false: Random forests is a type of bagging.", "answer": "RF uses bagging with feature randomness. ** True.\n#### True"}
{"question": "What is the bias-variance tradeoff regarding model complexity?", "answer": "Relate complexity to under/overfitting. ** Low complexity leads to underfitting (high bias, low variance). High complexity leads to overfitting (low bias, high variance).\n#### Low complexity=High Bias; High complexity=High Variance"}
{"question": "In a bagging ensemble for a regression task, how do you pick the final predictor?", "answer": "How are results combined in regression? ** By averaging the values of the predictions.\n#### Averaging"}
{"question": "True or false: SGD is less likely to get stuck in local minima than full batch gradient descent.", "answer": "Does the noise in SGD help? ** True. SGD is less stable/noisy, which can help it escape local minima.\n#### True"}
{"question": "What is the problem with only stacking linear layers in an MLP without non-linearities?", "answer": "Mathematical properties of linear operators. ** Stacking linear layers is equivalent to a single linear layer, so it gains no expressive power over a simple perceptron.\n#### Equivalent to a single linear layer"}
{"question": "What is the vanishing gradient problem?", "answer": "Effect of chain rule on deep networks. ** It is when the combination of gradients in earlier layers becomes very small (because gradients are < 1), making it difficult to update weights in early layers.\n#### Gradients become too small to update early layers effectively"}
{"question": "True or false: Convolutional layers are needed to reduce the dimensions of the input image data.", "answer": "What is the primary function of convolution vs pooling? ** False. Convolution applies filters; size reduction is typically done by pooling layers.\n#### False"}
{"question": "True or false: ResNet skip connections improve gradient flow during training.", "answer": "Purpose of residual connections. ** True. They allow the error gradient to propagate more efficiently to all layers.\n#### True"}
{"question": "True or false: Batch normalization is done over the whole dataset.", "answer": "Batch norm operation context. ** False. It is done over batches (mini-batches) for computational efficiency with SGD.\n#### False"}
{"question": "Order the methods from best to worst when you have scarce new data: Fine-tuning, Train from scratch, Linear probe.", "answer": "Map data availability to model complexity needs. ** 1. Linear probe (best for scarce data to avoid overfitting), 2. Fine-tuning, 3. Train from scratch (requires most data).\n#### Linear probe, Fine-tuning, Train from scratch"}
{"question": "True or false: Word2Vec dimensions must increase when adding new words.", "answer": "Vector space properties. ** False. The vector dimension is fixed; adding a word just adds a new vector of that fixed size.\n#### False"}
{"question": "True or false: The attention mechanism allows the model to focus on different parts of the input sequence.", "answer": "Function of attention. ** True (Self-attention).\n#### True"}
{"question": "Solve the Word2Vec analogy: 'Microsoft' - 'Bill Gates' + 'Jeff Bezos' = ?", "answer": "Company - Founder + Founder = Company. ** Amazon.\n#### Amazon"}
{"question": "True or false: BERT is a bidirectional model.", "answer": "BERT architecture. ** True. It considers surrounding words (before and after) simultaneously.\n#### True"}
{"question": "True or false: Vision Transformers (ViT) allow pixels to be directly compared to distant parts of the image.", "answer": "Contrast ViT global attention with CNN local fields. ** True. Unlike CNNs which are local, ViT allows patches to attend to distant patches.\n#### True"}
{"question": "True or false: Zero-shot learning is the ability to correctly predict data from a distribution different from the training one.", "answer": "Definition of zero-shot. ** False. Zero-shot is the ability to perform tasks/classes the model was not explicitly trained for (unseen classes), not just domain shift.\n#### False"}
{"question": "If a robot takes too long to complete a task in RL, what can you adjust in the reward function?", "answer": "Time preference in RL. ** Decrease the discount rate. This makes future rewards less valuable, incentivizing the robot to reach the goal (reward) faster.\n#### Decrease the discount rate"}